====setup creation====
====training inversion model====
Using gpu device 0: Tesla K80 (CNMeM is enabled with initial size: 50.0% of memory, cuDNN 5105)
Computing spectra...
====Training Files====
File 61/61 (100.00%) (took 1:18) 
====Validation Files====
File 16/16 (100.00%) (took 0:19) 
Loading labels...
Preparing training data feed...
Preparing prediction and generator functions...
/import/linux/DEEP_LEARNING/cuda-8-0/c4dm-theano/lib/python2.7/site-packages/lasagne/layers/pool.py:266: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.
  mode=self.mode,
/import/linux/DEEP_LEARNING/cuda-8-0/c4dm-theano/lib/python2.7/site-packages/lasagne/layers/pool.py:266: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.
  mode=self.mode,
/import/linux/DEEP_LEARNING/cuda-8-0/c4dm-theano/lib/python2.7/site-packages/lasagne/layers/pool.py:266: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.
  mode=self.mode,
Compiling prediction function...
Compiling training function...
Compiling validation function...
Training the inversion network:
Learning rate epoch 0: 0.001000
Epoch 1/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.653
Epoch 1/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.540
Training loss: previous epoch: 0.000000 current epoch:0.653049
Decaying lr, based on schedule 0
Learning rate epoch 1: 0.001000
Epoch 2/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.604
Epoch 2/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.533
Training loss: previous epoch: 0.653049 current epoch:0.604378
Decaying lr, based on schedule 0
Learning rate epoch 2: 0.001000
Epoch 3/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.598
Epoch 3/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.523
Training loss: previous epoch: 0.604378 current epoch:0.598058
Decaying lr, based on schedule 0
Learning rate epoch 3: 0.001000
Epoch 4/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.594
Epoch 4/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.547
Training loss: previous epoch: 0.598058 current epoch:0.593516
Decaying lr, based on schedule 0
Learning rate epoch 4: 0.001000
Epoch 5/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.591
Epoch 5/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.534
Training loss: previous epoch: 0.593516 current epoch:0.590530
Decaying lr, based on schedule 0
Learning rate epoch 5: 0.001000
Epoch 6/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.588
Epoch 6/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.526
Training loss: previous epoch: 0.590530 current epoch:0.588398
Decaying lr, based on schedule 0
Learning rate epoch 6: 0.001000
Epoch 7/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.587
Epoch 7/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.527
Training loss: previous epoch: 0.588398 current epoch:0.586734
Decaying lr, based on schedule 0
Learning rate epoch 7: 0.001000
Epoch 8/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.585
Epoch 8/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.525
Training loss: previous epoch: 0.586734 current epoch:0.584578
Decaying lr, based on schedule 0
Learning rate epoch 8: 0.001000
Epoch 9/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.583
Epoch 9/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.515
Training loss: previous epoch: 0.584578 current epoch:0.583087
Decaying lr, based on schedule 0
Learning rate epoch 9: 0.001000
Epoch 10/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.581
Epoch 10/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.517
Training loss: previous epoch: 0.583087 current epoch:0.581396
Decaying lr, based on schedule 0
Learning rate epoch 10: 0.001000
Epoch 11/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.580
Epoch 11/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.526
Training loss: previous epoch: 0.581396 current epoch:0.579675
Decaying lr, based on schedule 0
Learning rate epoch 11: 0.001000
Epoch 12/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.579
Epoch 12/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.524
Training loss: previous epoch: 0.579675 current epoch:0.579324
Decaying lr, based on schedule 0
Learning rate epoch 12: 0.001000
Epoch 13/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.578
Epoch 13/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.503
Training loss: previous epoch: 0.579324 current epoch:0.578243
Decaying lr, based on schedule 0
Learning rate epoch 13: 0.001000
Epoch 14/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.577
Epoch 14/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.521
Training loss: previous epoch: 0.578243 current epoch:0.577295
Decaying lr, based on schedule 0
Learning rate epoch 14: 0.001000
Epoch 15/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.577
Epoch 15/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.531
Training loss: previous epoch: 0.577295 current epoch:0.576590
Decaying lr, based on schedule 0
Learning rate epoch 15: 0.001000
Epoch 16/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.576
Epoch 16/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.513
Training loss: previous epoch: 0.576590 current epoch:0.575533
Decaying lr, based on schedule 0
Learning rate epoch 16: 0.001000
Epoch 17/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.575
Epoch 17/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.512
Training loss: previous epoch: 0.575533 current epoch:0.574929
Decaying lr, based on schedule 0
Learning rate epoch 17: 0.001000
Epoch 18/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.574
Epoch 18/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.518
Training loss: previous epoch: 0.574929 current epoch:0.574430
Decaying lr, based on schedule 0
Learning rate epoch 18: 0.001000
Epoch 19/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.574
Epoch 19/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.517
Training loss: previous epoch: 0.574430 current epoch:0.573682
Decaying lr, based on schedule 0
Learning rate epoch 19: 0.001000
Epoch 20/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.573
Epoch 20/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.510
Training loss: previous epoch: 0.573682 current epoch:0.573435
Decaying lr, based on schedule 0
Learning rate epoch 20: 0.001000
Epoch 21/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.573
Epoch 21/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.528
Training loss: previous epoch: 0.573435 current epoch:0.573104
Decaying lr, based on schedule 0
Learning rate epoch 21: 0.001000
Epoch 22/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.572
Epoch 22/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.515
Training loss: previous epoch: 0.573104 current epoch:0.572028
Decaying lr, based on schedule 0
Learning rate epoch 22: 0.001000
Epoch 23/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.572
Epoch 23/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.515
Training loss: previous epoch: 0.572028 current epoch:0.571586
Decaying lr, based on schedule 0
Learning rate epoch 23: 0.001000
Epoch 24/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.572
Epoch 24/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.508
Training loss: previous epoch: 0.571586 current epoch:0.571732
Decaying lr, based on schedule 0
Learning rate epoch 24: 0.001000
Epoch 25/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.571
Epoch 25/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.523
Training loss: previous epoch: 0.571732 current epoch:0.570828
Decaying lr, based on schedule 0
Learning rate epoch 25: 0.001000
Epoch 26/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.571
Epoch 26/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.513
Training loss: previous epoch: 0.570828 current epoch:0.570575
Decaying lr, based on schedule 0
Learning rate epoch 26: 0.001000
Epoch 27/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.571
Epoch 27/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.517
Training loss: previous epoch: 0.570575 current epoch:0.570572
Decaying lr, based on schedule 0
Learning rate epoch 27: 0.001000
Epoch 28/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.569
Epoch 28/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.513
Training loss: previous epoch: 0.570572 current epoch:0.569319
Decaying lr, based on schedule 0
Learning rate epoch 28: 0.001000
Epoch 29/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.570
Epoch 29/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.513
Training loss: previous epoch: 0.569319 current epoch:0.569574
Decaying lr, based on schedule 0
Learning rate epoch 29: 0.001000
Epoch 30/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.569
Epoch 30/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.506
Training loss: previous epoch: 0.569574 current epoch:0.569314
Decaying lr, based on schedule 0
Saving final model
====training ends====
====training inversion model====
Using gpu device 0: Tesla K80 (CNMeM is enabled with initial size: 50.0% of memory, cuDNN 5105)
Computing spectra...
====Training Files====
File 61/61 (100.00%) (took 1:20) 
====Validation Files====
File 16/16 (100.00%) (took 0:19) 
Loading labels...
Preparing training data feed...
Preparing prediction and generator functions...
/import/linux/DEEP_LEARNING/cuda-8-0/c4dm-theano/lib/python2.7/site-packages/lasagne/layers/pool.py:266: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.
  mode=self.mode,
/import/linux/DEEP_LEARNING/cuda-8-0/c4dm-theano/lib/python2.7/site-packages/lasagne/layers/pool.py:266: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.
  mode=self.mode,
/import/linux/DEEP_LEARNING/cuda-8-0/c4dm-theano/lib/python2.7/site-packages/lasagne/layers/pool.py:266: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.
  mode=self.mode,
Compiling prediction function...
Compiling training function...
Compiling validation function...
Training the inversion network:
Learning rate epoch 0: 0.100000
Epoch 1/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.665
Epoch 1/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.544
Training loss: previous epoch: 0.000000 current epoch:0.664847
Decaying lr, based on schedule 1
Learning rate epoch 1: 0.010000
Epoch 2/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.606
Epoch 2/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.546
Training loss: previous epoch: 0.664847 current epoch:0.605620
Decaying lr, based on schedule 1
Learning rate epoch 2: 0.001000
Epoch 3/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 3/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.528
Training loss: previous epoch: 0.605620 current epoch:0.599326
Decaying lr, based on schedule 1
Learning rate epoch 3: 0.000100
Epoch 4/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 4/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.523
Training loss: previous epoch: 0.599326 current epoch:0.598799
Decaying lr, based on schedule 1
Learning rate epoch 4: 0.000010
Epoch 5/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 5/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.543
Training loss: previous epoch: 0.598799 current epoch:0.598940
Decaying lr, based on schedule 1
Learning rate epoch 5: 0.000001
Epoch 6/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 6/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.511
Training loss: previous epoch: 0.598940 current epoch:0.599010
Decaying lr, based on schedule 1
Learning rate epoch 6: 0.000000
Epoch 7/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 7/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.531
Training loss: previous epoch: 0.599010 current epoch:0.599089
Decaying lr, based on schedule 1
Learning rate epoch 7: 0.000000
Epoch 8/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.598
Epoch 8/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.535
Training loss: previous epoch: 0.599089 current epoch:0.598496
Decaying lr, based on schedule 1
Learning rate epoch 8: 0.000000
Epoch 9/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 9/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.528
Training loss: previous epoch: 0.598496 current epoch:0.599181
Decaying lr, based on schedule 1
Learning rate epoch 9: 0.000000
Epoch 10/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 10/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.530
Training loss: previous epoch: 0.599181 current epoch:0.598787
Decaying lr, based on schedule 1
Learning rate epoch 10: 0.000000
Epoch 11/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 11/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.522
Training loss: previous epoch: 0.598787 current epoch:0.599169
Decaying lr, based on schedule 1
Learning rate epoch 11: 0.000000
Epoch 12/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 12/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.526
Training loss: previous epoch: 0.599169 current epoch:0.599443
Decaying lr, based on schedule 1
Learning rate epoch 12: 0.000000
Epoch 13/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 13/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.528
Training loss: previous epoch: 0.599443 current epoch:0.599241
Decaying lr, based on schedule 1
Learning rate epoch 13: 0.000000
Epoch 14/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 14/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.532
Training loss: previous epoch: 0.599241 current epoch:0.598915
Decaying lr, based on schedule 1
Learning rate epoch 14: 0.000000
Epoch 15/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 15/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.522
Training loss: previous epoch: 0.598915 current epoch:0.598958
Decaying lr, based on schedule 1
Learning rate epoch 15: 0.000000
Epoch 16/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.600
Epoch 16/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.530
Training loss: previous epoch: 0.598958 current epoch:0.599615
Decaying lr, based on schedule 1
Learning rate epoch 16: 0.000000
Epoch 17/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 17/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.531
Training loss: previous epoch: 0.599615 current epoch:0.598920
Decaying lr, based on schedule 1
Learning rate epoch 17: 0.000000
Epoch 18/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 18/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.524
Training loss: previous epoch: 0.598920 current epoch:0.599103
Decaying lr, based on schedule 1
Learning rate epoch 18: 0.000000
Epoch 19/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 19/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.537
Training loss: previous epoch: 0.599103 current epoch:0.599067
Decaying lr, based on schedule 1
Learning rate epoch 19: 0.000000
Epoch 20/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 20/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.531
Training loss: previous epoch: 0.599067 current epoch:0.598983
Decaying lr, based on schedule 1
Learning rate epoch 20: 0.000000
Epoch 21/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 21/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.511
Training loss: previous epoch: 0.598983 current epoch:0.599059
Decaying lr, based on schedule 1
Learning rate epoch 21: 0.000000
Epoch 22/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 22/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.543
Training loss: previous epoch: 0.599059 current epoch:0.598840
Decaying lr, based on schedule 1
Learning rate epoch 22: 0.000000
Epoch 23/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 23/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.529
Training loss: previous epoch: 0.598840 current epoch:0.599347
Decaying lr, based on schedule 1
Learning rate epoch 23: 0.000000
Epoch 24/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 24/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.518
Training loss: previous epoch: 0.599347 current epoch:0.598985
Decaying lr, based on schedule 1
Learning rate epoch 24: 0.000000
Epoch 25/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 25/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.532
Training loss: previous epoch: 0.598985 current epoch:0.598918
Decaying lr, based on schedule 1
Learning rate epoch 25: 0.000000
Epoch 26/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 26/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.529
Training loss: previous epoch: 0.598918 current epoch:0.599159
Decaying lr, based on schedule 1
Learning rate epoch 26: 0.000000
Epoch 27/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.600
Epoch 27/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.535
Training loss: previous epoch: 0.599159 current epoch:0.599534
Decaying lr, based on schedule 1
Learning rate epoch 27: 0.000000
Epoch 28/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 28/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.522
Training loss: previous epoch: 0.599534 current epoch:0.599268
Decaying lr, based on schedule 1
Learning rate epoch 28: 0.000000
Epoch 29/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 29/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.531
Training loss: previous epoch: 0.599268 current epoch:0.599111
Decaying lr, based on schedule 1
Learning rate epoch 29: 0.000000
Epoch 30/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.599
Epoch 30/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.525
Training loss: previous epoch: 0.599111 current epoch:0.599324
Decaying lr, based on schedule 1
Saving final model
====training ends====
====training inversion model====
Using gpu device 0: Tesla K80 (CNMeM is enabled with initial size: 50.0% of memory, cuDNN 5105)
Computing spectra...
====Training Files====
File 61/61 (100.00%) (took 1:20) 
====Validation Files====
File 16/16 (100.00%) (took 0:19) 
Loading labels...
Preparing training data feed...
Preparing prediction and generator functions...
/import/linux/DEEP_LEARNING/cuda-8-0/c4dm-theano/lib/python2.7/site-packages/lasagne/layers/pool.py:266: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.
  mode=self.mode,
/import/linux/DEEP_LEARNING/cuda-8-0/c4dm-theano/lib/python2.7/site-packages/lasagne/layers/pool.py:266: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.
  mode=self.mode,
/import/linux/DEEP_LEARNING/cuda-8-0/c4dm-theano/lib/python2.7/site-packages/lasagne/layers/pool.py:266: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.
  mode=self.mode,
Compiling prediction function...
Compiling training function...
Compiling validation function...
Training the inversion network:
Learning rate epoch 0: 0.001000
Epoch 1/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.643
Epoch 1/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.546
Training loss: previous epoch: 0.000000 current epoch:0.643404
Learning rate epoch 1: 0.001000
Epoch 2/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.603
Epoch 2/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.516
Training loss: previous epoch: 0.643404 current epoch:0.603178
Learning rate epoch 2: 0.001000
Epoch 3/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.597
Epoch 3/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.523
Training loss: previous epoch: 0.603178 current epoch:0.596788
Learning rate epoch 3: 0.001000
Epoch 4/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.592
Epoch 4/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.545
Training loss: previous epoch: 0.596788 current epoch:0.592304
Learning rate epoch 4: 0.001000
Epoch 5/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.589
Epoch 5/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.530
Training loss: previous epoch: 0.592304 current epoch:0.589133
Learning rate epoch 5: 0.001000
Epoch 6/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.586
Epoch 6/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.530
Training loss: previous epoch: 0.589133 current epoch:0.586468
Learning rate epoch 6: 0.001000
Epoch 7/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.585
Epoch 7/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.518
Training loss: previous epoch: 0.586468 current epoch:0.584621
Learning rate epoch 7: 0.001000
Epoch 8/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.582
Epoch 8/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.527
Training loss: previous epoch: 0.584621 current epoch:0.582436
Learning rate epoch 8: 0.001000
Epoch 9/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.581
Epoch 9/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.531
Training loss: previous epoch: 0.582436 current epoch:0.581209
Learning rate epoch 9: 0.001000
Epoch 10/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.580
Epoch 10/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.519
Training loss: previous epoch: 0.581209 current epoch:0.580482
Learning rate epoch 10: 0.001000
Epoch 11/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.580
Epoch 11/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.538
Training loss: previous epoch: 0.580482 current epoch:0.579652
Learning rate epoch 11: 0.001000
Epoch 12/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.578
Epoch 12/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.506
Training loss: previous epoch: 0.579652 current epoch:0.577965
Learning rate epoch 12: 0.001000
Epoch 13/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.577
Epoch 13/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.525
Training loss: previous epoch: 0.577965 current epoch:0.577344
Learning rate epoch 13: 0.001000
Epoch 14/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.577
Epoch 14/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.517
Training loss: previous epoch: 0.577344 current epoch:0.577057
Learning rate epoch 14: 0.001000
Epoch 15/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.576
Epoch 15/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.512
Training loss: previous epoch: 0.577057 current epoch:0.576332
Learning rate epoch 15: 0.001000
Epoch 16/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.575
Epoch 16/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.523
Training loss: previous epoch: 0.576332 current epoch:0.575205
Decaying lr, based on schedule 2
Learning rate epoch 16: 0.000500
Epoch 17/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.571
Epoch 17/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.508
Training loss: previous epoch: 0.575205 current epoch:0.570626
Learning rate epoch 17: 0.000500
Epoch 18/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.570
Epoch 18/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.520
Training loss: previous epoch: 0.570626 current epoch:0.570124
Learning rate epoch 18: 0.000500
Epoch 19/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.570
Epoch 19/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.512
Training loss: previous epoch: 0.570124 current epoch:0.569674
Learning rate epoch 19: 0.000500
Epoch 20/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.569
Epoch 20/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.511
Training loss: previous epoch: 0.569674 current epoch:0.568820
Learning rate epoch 20: 0.000500
Epoch 21/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.569
Epoch 21/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.527
Training loss: previous epoch: 0.568820 current epoch:0.568940
Learning rate epoch 21: 0.000500
Epoch 22/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.568
Epoch 22/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.507
Training loss: previous epoch: 0.568940 current epoch:0.568294
Learning rate epoch 22: 0.000500
Epoch 23/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.568
Epoch 23/30: Batch 792/792 (100.00%) (took 0:17) 
Validation loss: 0.510
Training loss: previous epoch: 0.568294 current epoch:0.568088
Decaying lr, based on schedule 2
Learning rate epoch 23: 0.000250
Epoch 24/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.566
Epoch 24/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.506
Training loss: previous epoch: 0.568088 current epoch:0.565878
Learning rate epoch 24: 0.000250
Epoch 25/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.566
Epoch 25/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.518
Training loss: previous epoch: 0.565878 current epoch:0.565526
Learning rate epoch 25: 0.000250
Epoch 26/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.565
Epoch 26/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.524
Training loss: previous epoch: 0.565526 current epoch:0.565159
Learning rate epoch 26: 0.000250
Epoch 27/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.565
Epoch 27/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.507
Training loss: previous epoch: 0.565159 current epoch:0.564986
Learning rate epoch 27: 0.000250
Epoch 28/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.565
Epoch 28/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.510
Training loss: previous epoch: 0.564986 current epoch:0.564812
Learning rate epoch 28: 0.000250
Epoch 29/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.565
Epoch 29/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.514
Training loss: previous epoch: 0.564812 current epoch:0.564573
Learning rate epoch 29: 0.000250
Epoch 30/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.565
Epoch 30/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.506
Training loss: previous epoch: 0.564573 current epoch:0.564614
Saving final model
====training ends====
====training inversion model====
Using gpu device 0: Tesla K80 (CNMeM is enabled with initial size: 50.0% of memory, cuDNN 5105)
Computing spectra...
====Training Files====
File 61/61 (100.00%) (took 1:19) 
====Validation Files====
File 16/16 (100.00%) (took 0:19) 
Loading labels...
Preparing training data feed...
Preparing prediction and generator functions...
/import/linux/DEEP_LEARNING/cuda-8-0/c4dm-theano/lib/python2.7/site-packages/lasagne/layers/pool.py:266: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.
  mode=self.mode,
/import/linux/DEEP_LEARNING/cuda-8-0/c4dm-theano/lib/python2.7/site-packages/lasagne/layers/pool.py:266: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.
  mode=self.mode,
/import/linux/DEEP_LEARNING/cuda-8-0/c4dm-theano/lib/python2.7/site-packages/lasagne/layers/pool.py:266: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.
  mode=self.mode,
Compiling prediction function...
Compiling training function...
Compiling validation function...
Training the inversion network:
Learning rate epoch 0: 0.001000
Epoch 1/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.654
Epoch 1/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.541
Training loss: previous epoch: 0.000000 current epoch:0.654027
Learning rate epoch 1: 0.001000
Epoch 2/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.604
Epoch 2/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.534
Training loss: previous epoch: 0.654027 current epoch:0.603738
Learning rate epoch 2: 0.001000
Epoch 3/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.597
Epoch 3/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.543
Training loss: previous epoch: 0.603738 current epoch:0.596677
Learning rate epoch 3: 0.001000
Epoch 4/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.592
Epoch 4/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.504
Training loss: previous epoch: 0.596677 current epoch:0.591809
Decaying lr, based on schedule 3
Learning rate epoch 4: 0.000500
Epoch 5/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.585
Epoch 5/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.528
Training loss: previous epoch: 0.591809 current epoch:0.584902
Learning rate epoch 5: 0.000500
Epoch 6/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.583
Epoch 6/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.523
Training loss: previous epoch: 0.584902 current epoch:0.582905
Decaying lr, based on schedule 3
Learning rate epoch 6: 0.000250
Epoch 7/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.579
Epoch 7/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.518
Training loss: previous epoch: 0.582905 current epoch:0.578986
Learning rate epoch 7: 0.000250
Epoch 8/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.578
Epoch 8/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.510
Training loss: previous epoch: 0.578986 current epoch:0.577826
Decaying lr, based on schedule 3
Learning rate epoch 8: 0.000125
Epoch 9/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.575
Epoch 9/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.519
Training loss: previous epoch: 0.577826 current epoch:0.575046
Decaying lr, based on schedule 3
Learning rate epoch 9: 0.000063
Epoch 10/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.575
Epoch 10/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.509
Training loss: previous epoch: 0.575046 current epoch:0.574795
Decaying lr, based on schedule 3
Learning rate epoch 10: 0.000031
Epoch 11/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.574
Epoch 11/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.517
Training loss: previous epoch: 0.574795 current epoch:0.574367
Decaying lr, based on schedule 3
Learning rate epoch 11: 0.000016
Epoch 12/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.573
Epoch 12/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.516
Training loss: previous epoch: 0.574367 current epoch:0.573442
Decaying lr, based on schedule 3
Learning rate epoch 12: 0.000008
Epoch 13/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.574
Epoch 13/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.512
Training loss: previous epoch: 0.573442 current epoch:0.573538
Decaying lr, based on schedule 3
Learning rate epoch 13: 0.000004
Epoch 14/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.574
Epoch 14/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.508
Training loss: previous epoch: 0.573538 current epoch:0.573754
Decaying lr, based on schedule 3
Learning rate epoch 14: 0.000002
Epoch 15/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.574
Epoch 15/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.507
Training loss: previous epoch: 0.573754 current epoch:0.573668
Decaying lr, based on schedule 3
Learning rate epoch 15: 0.000001
Epoch 16/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.574
Epoch 16/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.533
Training loss: previous epoch: 0.573668 current epoch:0.573781
Decaying lr, based on schedule 3
Learning rate epoch 16: 0.000000
Epoch 17/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.573
Epoch 17/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.509
Training loss: previous epoch: 0.573781 current epoch:0.573344
Decaying lr, based on schedule 3
Learning rate epoch 17: 0.000000
Epoch 18/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.573
Epoch 18/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.515
Training loss: previous epoch: 0.573344 current epoch:0.573469
Decaying lr, based on schedule 3
Learning rate epoch 18: 0.000000
Epoch 19/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.574
Epoch 19/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.512
Training loss: previous epoch: 0.573469 current epoch:0.573670
Decaying lr, based on schedule 3
Learning rate epoch 19: 0.000000
Epoch 20/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.574
Epoch 20/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.508
Training loss: previous epoch: 0.573670 current epoch:0.573609
Decaying lr, based on schedule 3
Learning rate epoch 20: 0.000000
Epoch 21/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.574
Epoch 21/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.526
Training loss: previous epoch: 0.573609 current epoch:0.573679
Decaying lr, based on schedule 3
Learning rate epoch 21: 0.000000
Epoch 22/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.573
Epoch 22/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.506
Training loss: previous epoch: 0.573679 current epoch:0.573116
Decaying lr, based on schedule 3
Learning rate epoch 22: 0.000000
Epoch 23/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.573
Epoch 23/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.523
Training loss: previous epoch: 0.573116 current epoch:0.573450
Decaying lr, based on schedule 3
Learning rate epoch 23: 0.000000
Epoch 24/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.574
Epoch 24/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.513
Training loss: previous epoch: 0.573450 current epoch:0.573620
Decaying lr, based on schedule 3
Learning rate epoch 24: 0.000000
Epoch 25/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.574
Epoch 25/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.504
Training loss: previous epoch: 0.573620 current epoch:0.573520
Decaying lr, based on schedule 3
Learning rate epoch 25: 0.000000
Epoch 26/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.573
Epoch 26/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.532
Training loss: previous epoch: 0.573520 current epoch:0.573425
Decaying lr, based on schedule 3
Learning rate epoch 26: 0.000000
Epoch 27/30: Batch 3111/3111 (100.00%) (took 1:31) 
Train loss: 0.574
Epoch 27/30: Batch 792/792 (100.00%) (took 0:17) 
Validation loss: 0.503
Training loss: previous epoch: 0.573425 current epoch:0.573707
Decaying lr, based on schedule 3
Learning rate epoch 27: 0.000000
Epoch 28/30: Batch 3111/3111 (100.00%) (took 1:30) 
Train loss: 0.573
Epoch 28/30: Batch 792/792 (100.00%) (took 0:18) 
Validation loss: 0.511
Training loss: previous epoch: 0.573707 current epoch:0.573289
Decaying lr, based on schedule 3
Learning rate epoch 28: 0.000000
Epoch 29/30: Batch 3111/3111 (100.00%) (took 1:30) 
Train loss: 0.573
Epoch 29/30: Batch 792/792 (100.00%) (took 0:17) 
Validation loss: 0.509
Training loss: previous epoch: 0.573289 current epoch:0.573451
Decaying lr, based on schedule 3
Learning rate epoch 29: 0.000000
Epoch 30/30: Batch 3111/3111 (100.00%) (took 1:30) 
Train loss: 0.574
Epoch 30/30: Batch 792/792 (100.00%) (took 0:17) 
Validation loss: 0.518
Training loss: previous epoch: 0.573451 current epoch:0.573742
Decaying lr, based on schedule 3
Saving final model
====training ends====
(c4dm-theano)bash-4.2$ 

